# WriteId
书写者识别任务

## 2019.5.23
1. 完成数据的处理工作，每个训练样本为100个RHS
2. 完成Data10基本框架，测试集精度达83%，训练集达92%：
  - 6层LSTM堆叠
  - batch_size=800, Adam lr=0.001, epochs=60
3. TODO:
  - 精度提升，参数初始化等
 
## 2019.5.24
1. 完成测试脚本，问题：单个 100RHS 测试准确率低（由于分类顺序的问题，已解决，单个80%左右），TODO：多个RHS共同判断 5-10个RHS可以达到100%精度
2. 测试不同层数
  - 6层，batch_size=1000, 70epochs，训练集精度：97.87%，测试集85.85%，模型rnn6,Dropout=0.5
  - 8层，batch_size=800，基本学不到东西，可能由于层数太多，梯度消失
  
## 2019.5.27
1. 100分类任务：
  双向LSTM，3层，隐藏层400(x2)，精度可以得到有效提升
  batchsize=2000，Adam加入权重衰减0.0005

## 2019.5.28
1. 数据清洗：
  在每个人生成RHS时，若样本中笔画数仅为1且笔画位置仅为1，即该汉字只录入了一个点，舍弃。已完成。
2. 可以考虑对数据文件生成索引文件，否则每次数据读取耗时较多（但也无所谓...）
3. 重新对清新后数据和调整数据后数据进行训练，记录参数
  - Task10: 60epochs
    - 每个人样本的序列长度：100，模型保存名rnn5.pkl(被误删了TAT)，曲线图：10-5layer-1000ba.jpg
    - 5层单向，隐藏层=800，batch_size=1000，训练样本数1000，测试600，Adam学习率0.001，权重衰减0.0005（在后期降低学习率）
    - 准确率：训练：89.01%，测试 81.30%， Loss：训练：0.2887，测试：0.4891
    
    70epochs
    - 每个人样本的序列长度：100，模型保存名rnn6.pkl，曲线图：10-6layer-1000ba.jpg
    - 6层单向，隐藏层=800，batch_size=1000，训练样本数1000，测试600，Adam学习率0.001，权重衰减0.0005（在后期降低学习率）
    - 准确率：训练：90.57%，测试 82.93%， Loss：训练：0.2574，测试：0.4415
    
    测试效果：
    - 2018310898与2018310874是比较容易分错的，是否可以对错误样本做可视化分析？？？#TODO，其他类别还行，取10个基本能保证90%的正确率。
    
    对于2018310898的测试样本的判断结果: 应该为8，而5占大多数，判断错误
    ```
    tensor([5, 5, 8, 5, 8, 8, 5, 8, 5, 5, 8, 8, 5, 5, 5, 5, 8, 2, 5, 5, 8, 2, 8, 5,
        5, 8, 5, 8, 5, 5, 8, 5, 8, 5, 8, 8, 8, 8, 5, 8, 8, 5, 5, 8, 5, 8, 8, 2,
        8, 5, 5, 5, 5, 8, 5, 8, 8, 5, 5, 8, 8, 5, 8, 5, 5, 5, 8, 5, 2, 8, 8, 5,
        5, 5, 5, 8, 8, 5, 8, 2, 5, 5, 8, 8, 8, 8, 8, 5, 5, 5, 5, 5, 5, 5, 5, 5,
        8, 5, 8, 8], device='cuda:0')
    5
    ```
    
    对于2018310874的测试样本的判断结果:5依然占大多数，判断正确
    ```
    tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 8, 8, 5, 5, 5,
        5, 5, 8, 5, 5, 5, 5, 5, 5, 5, 5, 8, 5, 5, 5, 5, 5, 5, 5, 5, 8, 5, 5, 8,
        5, 5, 5, 8, 5, 8, 5, 6, 5, 8, 5, 5, 5, 5, 5, 8, 5, 5, 8, 5, 5, 8, 2, 5,
        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 8, 5, 5, 5, 8, 5, 5, 5, 5, 5, 5,
        5, 5, 5, 8], device='cuda:0')
    5
    ```
    60epochs
    - 每个人样本的序列长度：100，模型保存名10rnn1-bi.pkl，曲线图：10-1layer-bi.jpg
    - 1层双向，隐藏层=400，batch_size=1000，训练样本数1000，测试600，Adam学习率0.001，权重衰减0.0005（在后期降低学习率）
    - 准确率：训练：95.12%，测试 84.21%， Loss：训练：0.1463，测试：0.4825
    - 过拟合严重，加入防止过拟合
    测试效果：
    与单向6层相同，判断8样本时容易判断为5！
    
    对1层双向加0.5的Dropout，60epochs
    - 准确率：训练：95.49%，测试84.73%， Loss：训练：0.1287，测试：0.50
    提升不大？？
    - 考虑对类别5和8增加样本？
    
    对5和8两个类别增加样本，训练样本数为1600
     - 双向2层训练，50轮,0.5的Dropout
     - 准确率：训练：93.04%，测试：86.30%， Loss：训练：0.1915，测试：0.3829
     - 感觉还能继续下降，在基础上继续训练看看
     - 准确率：训练：96.85%，测试：87.00%， Loss：训练：0.0915，测试：0.446
     - **感觉应该要用其他的特征避免5,8混淆，但在测试阶段取10个RHS基本能保证分对，仍有错分现象。**
    
  - Task100：60epochs
    - 每个人样本的序列长度：100，模型保存名100rnn3.pkl，曲线图：100-1.jpg
    - 3层双向，隐藏层=400，batch_size=1000，训练样本数1500，测试600，Adam学习率0.001，权重衰减0.0005（在后期降低学习率）
    - 准确率：训练：95.98%，测试：91.70%， Loss：训练：0.1213，测试：0.2532
    
    测试效果：取十个样本取多数，93%正确率，即判断有7个人错误
    错误分类
     ```
    (array([100, 101, 102, 103, 104, 105, 106]),)
    [2018311127 2018311146 2018312459 2018312470 2018312476 2018312481 2018312484]
    [2018211051 2018211080 2018210461 2018310898 2018310907 2018210461 2018310927]
    ```
    TAT发现是因为一共有107个类别，只做了100个人的分类，没看清题目和数据的后果...再训一次咯，证明前100个人均分类正确了！！
    - 107重新训练，上述参数不变
    - Train Loss: 0.142875, Train Acc: 0.952950, Eval Loss: 0.311687, Eval Acc: 0.900477
    
    测试效果：测试了5次，均为100%。
    
    
 ## 2019.5.28-Rui
 
1. 重新排序并生成Task10文件夹，取1间隔RHS生成Task10_2
   
2. 双向LSTM模型用于Task10
   - Task10: 70epochs
    - 特征:相邻点RHS
    - 每个人样本的序列长度：100，模型保存名rnn6.pkl
    - 6层双向，隐藏层=400，batch_size=1000，训练样本数1000，测试600，Adam学习率0.001，权重衰减0
    - 准确率： Loss：(忘记录了...不过和之前单层差不多)
    - 测试: 5次测试均90%，每次都是8分成5.
    - 结论：双向LSTM和单向效果差不多。
    
3. 双向LSTM模型和1间隔RHS用于Task10
   - Task10: 70epochs
    - 特征:1间隔RHS
    - 每个人样本的序列长度：100，模型保存名rnn_2.pkl
    - 6层双向，隐藏层=400，batch_size=1000，训练样本数1000，测试600，Adam学习率0.001，权重衰减0
    - Train10 Loss: 0.018413, Train10 Acc: 0.994100, Eval Loss: 0.592419, Eval Acc: 0.883500
    - 测试: 5次测试，1次90%，4次100%。90%是8分成5，但是看上去比上面的相邻RHS好一点。
    - 结论: 1间隔RHS仍包含有区分Writers的重要信息，效果不比相邻RHS差。观察其训练过程在epochs为36时测试正确率已经达到90%，之后直到70epochs都没有什么提升，只是训练正确率慢慢达到99%以上，可能有过拟合现象。
   - Task10: 60epochs
    - 特征:1间隔RHS
    - 每个人样本的序列长度：100，模型保存名rnn_2.pkl
    - 6层双向，隐藏层=400，batch_size=1000，训练样本数1000，测试600，Adam学习率0.001，权重衰减0.0005
    - epoch: 60, Train10 Loss: 0.145206, Train10 Acc: 0.953000, Eval Loss: 0.488143, Eval Acc: 0.857167
    - 测试: 5次测试，都是90%，问题都是是8分成5。
    - 结论: 加了decay的效果没有预想中的好，可能是epoch不太够？但是8分成5问题还是很明显，这次实验表明了1间隔RHS不能明显区分开5，8。但是突然好奇为什么5不会分为8，但是8会分成5？可能两个特征空间有交集，之后5的特征空间交集占比大，而8占比小？
   
4. 双向LSTM模型和2间隔RHS用于Task10
   - Task10: 60epochs
    - 特征:2间隔RHS
    - 每个人样本的序列长度：100，模型保存名rnn_3.pkl
    - 6层双向，隐藏层=400，batch_size=1000，训练样本数1000，测试600，Adam学习率0.001，权重衰减0.0001
    - Train10 Loss: 0.052118, Train10 Acc: 0.982000, Eval Loss: 0.482039, Eval Acc: 0.892167
    - 测试: 5次测试，2次90%，3次100%。90%是8分成5。
    - 结论: 2间隔RHS仍包含有区分Writers的重要信息...  
    
 ## 2019.5.29
 1. 加入CNN，直接用VGG11-bn与训练模型进行微调，过拟合严重，加入数据增强，随即反转和剪裁，依然严重
  - 模型名：vgg-11.pth, 曲线图：CNN-10vgg11.jpg
  - batch_size = 16, num_classes = 10 # 分类, num_epochs = 10, Adam:learning_rate = 0.0001
  - Train10 Loss: 0.088059, Train10 Acc: 0.973026, Eval Loss: 1.352584, Eval Acc: 0.646429
  - 可以写个测试脚本试试效果，毕竟在训练集上很低，看看是否有区分5,8能力
 
 2. 100分类输出测试，查看对10分类样本中的5:2018310874,8:2018310898，类别的分类能力：
  测试结果：
  ```
  预测： 学号:  2018310874 频次： 7
  预测： 学号:  2018211167 频次： 1
  预测： 学号:  2018310921 频次： 1
  预测： 学号:  2018211051 频次： 1
  真实学号： 2018310874
  
  预测： 学号:  2018310898 频次： 5
  预测： 学号:  2018211167 频次： 2
  预测： 学号:  2018312484 频次： 2
  预测： 学号:  2018310881 频次： 1
  真实学号： 2018310898
  ```
  - 100分类时对于898的样本仍有错分样本，但此时会与其他样本相似，导致判断为自身仍占多数，这是好呢还是不好呢？？
  - 考虑10个样本组合分类，将<8个判断正确的样本学号输出查看：
  ```
预测： 学号:  2015011548 频次： 7
预测： 学号:  2018310926 频次： 1
预测： 学号:  2018211063 频次： 1
预测： 学号:  2018211167 频次： 1
真实学号： 2015011548
预测： 学号:  2018211069 频次： 6
预测： 学号:  2018270032 频次： 2
预测： 学号:  2018211060 频次： 1
预测： 学号:  2018310769 频次： 1
真实学号： 2018211069
预测： 学号:  2018211081 频次： 6
预测： 学号:  2018310894 频次： 3
预测： 学号:  2018310942 频次： 1
真实学号： 2018211081
预测： 学号:  2018214052 频次： 7
预测： 学号:  2018214042 频次： 2
预测： 学号:  2018310907 频次： 1
真实学号： 2018214052
预测： 学号:  2018214113 频次： 7
预测： 学号:  2018211064 频次： 2
预测： 学号:  2015011414 频次： 1
真实学号： 2018214113
**预测： 学号:  2018310874 频次： 4**
预测： 学号:  2018310922 频次： 1
预测： 学号:  2018211167 频次： 1
预测： 学号:  2018310921 频次： 1
预测： 学号:  2018211080 频次： 1
预测： 学号:  2018311146 频次： 1
预测： 学号:  2018211702 频次： 1
真实学号： 2018310874
预测： 学号:  2018310881 频次： 7
预测： 学号:  2018211167 频次： 3
真实学号： 2018310881
预测： 学号:  2018310882 频次： 7
预测： 学号:  2018310897 频次： 1
预测： 学号:  2018214042 频次： 1
预测： 学号:  2018310883 频次： 1
真实学号： 2018310882
预测： 学号:  2018310883 频次： 6
预测： 学号:  2018310933 频次： 2
预测： 学号:  2018210809 频次： 1
预测： 学号:  2018214042 频次： 1
真实学号： 2018310883
预测： 学号:  2018310895 频次： 7
预测： 学号:  2018210461 频次： 1
预测： 学号:  2018312459 频次： 1
预测： 学号:  2015011455 频次： 1
真实学号： 2018310895
**预测： 学号:  2018310898 频次： 5**
预测： 学号:  2018310936 频次： 3
预测： 学号:  2018211167 频次： 1
预测： 学号:  2018312484 频次： 1
真实学号： 2018310898
预测： 学号:  2018310942 频次： 7
预测： 学号:  2015011431 频次： 1
预测： 学号:  2018211079 频次： 1
预测： 学号:  2018310894 频次： 1
真实学号： 2018310942
预测： 学号:  2018310946 频次： 6
预测： 学号:  2018310907 频次： 2
预测： 学号:  2018214042 频次： 1
预测： 学号:  2017310881 频次： 1
真实学号： 2018310946
  ```
  
  依然对于上述两个样本，会混入比较多其他人的特征，SAD
 
## 2019.5.29-Rui
 
1. 双向LSTM模型和4间隔RHS用于Task10
   - Task10: 60epochs
    - 特征:4间隔RHS
    - 每个人样本的序列长度：100，模型保存名rnn_5.pkl
    - 6层双向，隐藏层=400，batch_size=1000，训练样本数1000，测试600，Adam学习率0.001，权重衰减0.0001
    - Train10 Loss: 0.094468, Train10 Acc: 0.968500, Eval Loss: 0.552045, Eval Acc: 0.879167
    - 测试: 5次测试，1次90%，4次100%。90%是8分成5。
    - 结论: 4间隔RHS...
    
2. 在LSTM输出加softmax，取100RHS求概率和进行判断
   - 对**2019.5.28-Rui**生成的模型进行测试，结果是测试结果一直是90%的模型，还是90%，但是**之前在这两者之间摇摆的模型稳定为100%(2、4间隔RHS以及1RHs的no_decay版本)** ，因此这样改变还是有效果的。
    ```
    import torch.nn.functional as F
    out_softmax = F.softmax(out)
    out_softmax_sum = out_softmax.sum(0)
    _, pred = out_softmax_sum.max(0)
    ```
## 2019.5.30-Rui
 
1. 0、2、4间隔RHS混合训练
   - Task10: 70epochs
    - 特征:1、3、5RHS
    - 每个人样本的序列长度：100，模型保存名rnn_1_3_5.pkl
    - 3层双向，隐藏层=400，batch_size=1000，训练样本数1000，测试600，Adam学习率0.001，权重衰减0.0001
    - Train10 Loss: 0.011944, Train10 Acc: 0.997700, Eval Loss: 0.713745, Eval Acc: 0.848500
    - 测试: 还没测
    - 结论: 

## 2019.5.30-Siyu
1. 十分类，在双向LSTM加dropout的基础上进行微调，每个人的样本包含-1，-3，-5的RHS
  - epochs：60
  - 模型名：10rnn2-bi-drop135.pkl
  - Train10 Loss: 0.021462, Train10 Acc: 0.994448, Eval Loss: 0.338192, Eval Acc: 0.911000

  - 测试：
    - 采用该模型，仅对10个**RHS1**样本测试：仅对8的区分性不强，其他基本能稳定在8个正确
    ```
    预测： 学号:  8 2018310898 频次： 6
    预测： 学号:  5 2018310874 频次： 3
    预测： 学号:  9 2018310939 频次： 1
    Softmax 预测, 概率：  8 [0.0015, 0.0069, 0.1225, 0.0003, 0.3068, 2.2491, 0.0161, 0.0014, 6.1698, 1.1256]
    ```
    - 采用该模型，仅对10个**RHS3**样本测试：对7的区分性非常不强！，其他基本只有5-6个的样本，区分性不好
    ```
    预测： 学号:  3 2018211054 频次： 5
    预测： 学号:  4 2018211702 频次： 2
    预测： 学号:  6 2018310892 频次： 2
    预测： 学号:  0 2016310874 频次： 1
    Softmax 预测, 概率：  3 [1.0375, 0.0015, 0.0019, 4.9069, 1.898, 0.0074, 1.6084, 0.5379, 0.0002, 0.0004]
    ```
    - 采用该模型，仅对10个**RHS5**样本测试：
    如果用概率和为最高作为预测的值，偶尔也会出错
    ```
    预测： 学号:  8 2018310898 频次： 4
    预测： 学号:  5 2018310874 频次： 3
    预测： 学号:  2 2018211051 频次： 3
    Softmax 预测, 概率：  5 [0.0, 0.0046, 2.9616, 0.0003, 0.0052, 3.7831, 0.0062, 0.0002, 3.238, 0.0008]
    ```
    仅用频次似乎能避免错误..稳定100%
    ```
    预测： 学号:  6 2018310892 频次： 7
    预测： 学号:  5 2018310874 频次： 3
    Softmax 预测, 概率：  6 [0.0002, 0.0045, 0.0074, 0.0024, 0.0168, 2.7986, 7.1543, 0.0033, 0.0092, 0.0034]
    预测： 学号:  8 2018310898 频次： 7
    预测： 学号:  5 2018310874 频次： 3
    Softmax 预测, 概率：  8 [0.0, 0.0038, 0.0852, 0.0004, 0.002, 2.9919, 0.0176, 1e-04, 6.8986, 0.0004]
    ```
    - 采用该模型，三个样本每种10/5（均可）个共同评估：稳定100%，基本判断正确>20个样本，偶尔会有一点差错，但从概率层面来看区分度非常高！
  ```
预测： 学号:  8 2018310898 频次： 19
预测： 学号:  5 2018310874 频次： 9
预测： 学号:  9 2018310939 频次： 1
预测： 学号:  2 2018211051 频次： 1
Softmax 预测, 概率：  8 [0.0004, 0.0085, 1.53, 0.0005, 0.029, 7.2402, 0.0391, 0.0009, 20.1511, 1.0004]
(array([], dtype=int64),)

每种取5个
预测： 学号:  8 2018310898 频次： 6
预测： 学号:  5 2018310874 频次： 6
预测： 学号:  2 2018211051 频次： 3
Softmax 预测, 概率：  8 [0.0005, 0.0105, 3.4464, 0.0005, 0.0191, 4.7295, 0.0073, 0.0012, 6.2708, 0.5143] 
从概率上看更加合适
  ```

## 2019.6.1
  - 对新的RHS数据进行训练
  - 单独各长度实验，10,30,50,80,100
  - 混合长度实验
  - 已完成对LSTM对变长序列的处理
  - 已完成数据样本的索引生成
  - 10分类：双向两层LSTM，学习率30ecpo x 0.1，初始0.001，正则化参数0.0005
  - 参数初始化有问题，加上之后不收敛？
  - 具体：10分类
    - 100长度：名称：save/10rnn2-bi-new100.pkl
      `epoch: 30, Train10 Loss: 0.176822, Train10 Acc: 0.937900, Eval Loss: 0.292234, Eval Acc: 0.896200  -0.001学习率`
     ` epoch: 50, Train10 Loss: 0.091720, Train10 Acc: 0.975000, Eval Loss: 0.203464, Eval Acc: 0.930500  -0.0001学习率`
    - 10长度：名称：save/10rnn2-bi-new10.pkl 无dropout 双层双向
      `epoch: 50, Train10 Loss: 0.902014, Train10 Acc: 0.683100, Eval Loss: 1.191628, Eval Acc: 0.577000`
    - 30长度：名称：save/10rnn2-bi-new30.pkl 无dropout 单层双向，效果不好，有过拟合
      `epoch: 50, Train10 Loss: 0.744200, Train10 Acc: 0.738500, Eval Loss: 1.165802, Eval Acc: 0.589100`
      双层双向：
      `epoch: 50, Train10 Loss: 0.205758, Train10 Acc: 0.934400, Eval Loss: 0.571831, Eval Acc: 0.807400`
    - 50长度：双层双向 过拟合
      `epoch: 50, Train10 Loss: 0.143937, Train10 Acc: 0.953800, Eval Loss: 0.382054, Eval Acc: 0.869000`
      加入Dropout：
      `epoch: 50, Train10 Loss: 0.052370, Train10 Acc: 0.987000, Eval Loss: 0.361123, Eval Acc: 0.890900`
    - 80长度：
      `epoch: 50, Train10 Loss: 0.130974, Train10 Acc: 0.961400, Eval Loss: 0.304671, Eval Acc: 0.897400`
      加入Dropout
      `epoch: 50, Train10 Loss: 0.101151, Train10 Acc: 0.971400, Eval Loss: 0.256305, Eval Acc: 0.915100`
    - 结论，均有过拟合发生，考虑加入dropout，但似乎100长度的效果是最好的
      
    - 混合训练 50 80 100 在100的基础上 dropout=0.5   10rnn2-bi-new5080100.pkl
    `epoch: 50, Train10 Loss: 0.013527, Train10 Acc: 0.997133, Eval Loss: 0.149151, Eval Acc: 0.956967`
      30 80 10 也在100RHS的基础上
    `epoch: 50, Train10 Loss: 0.064725, Train10 Acc: 0.982700, Eval Loss: 0.348810, Eval Acc: 0.891700`
    
  - 107分类
    - 100RHS
    `epoch: 50, Train10 Loss: 0.019877, Train10 Acc: 0.997296, Eval Loss: 0.264691, Eval Acc: 0.925102`
    - 50RHS 
    `epoch: 50, Train10 Loss: 0.051595, Train10 Acc: 0.990034, Eval Loss: 0.478447, Eval Acc: 0.866919`
    训练太慢了，于是只打算训混合的和两个单独的
    - 5080100
    `Train10 Loss: 0.029824, Train10 Acc: 0.992840, Eval Loss: 0.236009, Eval Acc: 0.936400`
      
## 2019.6.2-Rui
 
1. 调研了attention
   
2. Attention用于书写着识别
   - Task10: 70epochs
    - 特征:相邻点RHS
    - 每个人样本的序列长度：100，模型保存名rnn_1_attention.pkl
    - 3层双向，隐藏层=400，batch_size=1000，训练样本数1000，测试600，Adam学习率0.001，权重衰减0.0001, attentino层一个隐含层24个节点。
    - epoch: 70, Train10 Loss: 0.056789, Train10 Acc: 0.983500, Eval Loss: 0.524559, Eval Acc: 0.867500
   
## 2019.6.3-Rui

1. Attention用于书写着识别
   - Task10: 60epochs
    - 特征:5RHS
    - 每个人样本的序列长度：100，模型保存名encoder_5_attention.pkl、classfiter_5_attention.pkl
    - 3层双向，隐藏层=400，batch_size=1000，训练样本数1000，测试600，Adam学习率0.001，权重衰减0.0001, attentino层一个隐含层64个节点。
    - epoch: 60, Train10 Loss: 0.017537, Train10 Acc: 0.995100, Eval Loss: 0.483785, Eval Acc: 0.881000
    - 特点: 正确率上升非常快，epoch: 21, Train10 Loss: 0.123209, Train10 Acc: 0.959400, Eval Loss: 0.309864, Eval Acc: 0.895333
    但是之后只有训练正确率上升，测试正确率波动，可能已经学不到什么东西了。
    - 结论：加入attention机制正确率没有显著提高的可能原因。关于LSTM的attention机制，比较流行的是针对seq2seq(encoder-decoder框架)任务，不同的decoder输出可以采用不同的encoder输出权重，使得不同的decoder输出**聚焦**于不同的部分。但是书写者识别任务是many2one任务的，因此我们要学习的只有针对one的many权重，但是回顾我们的特征，其实诸如100RHS之间是没有明确的顺序关系的，因为每个特征都是随机挑选出来的，因此可能前面学习了权重，但是由于特征提取的随机性，也难以正确聚焦。

2.TODO: visualization
  
